{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2b7192",
   "metadata": {},
   "source": [
    "# Part 5. 데이터 사전 처리\n",
    "\n",
    "머신러닝 등 데이터 분석의 정확도는 분석 데이터의 품질에 의해 좌우된다.  \n",
    "데이터의 품질을 높이기 위해서는 누락 데이터, 중복 데이터 등 오류를 수정하고 분석 목적에 맞게 변형하는 과정이 필요하다.  \n",
    "수집한 데이터를 분석에 적합하도록 사전 처리하는 방법을 살펴보자.  \n",
    "\n",
    "## 1. 누락 데이터 처리\n",
    "\n",
    "일반적으로 유효한 데이터 값이 존재하지 않는 누락 데이터를 NaN(Not a Number)으로 표시한다.  \n",
    "\n",
    "누락 데이터가 많아지면 데이터의 품질이 떨어지고, 머신러닝 분석 알고리즘을 왜곡하는 현상이 발생하기 때문에, 분석 하기 전 반드시 누락 데이터를 제거하거나 다른 적절한 값으로 대체하는 과정이 필요하다.\n",
    "\n",
    "* 누락 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a41a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dfa746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# 'deck' 열에 203 개의 유효한 데이터가 존재, 누락 데이터는 688(891-203)개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa08bd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    688\n",
       "C       59\n",
       "B       47\n",
       "D       33\n",
       "E       32\n",
       "A       15\n",
       "F       13\n",
       "G        4\n",
       "Name: deck, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_deck = df['deck'].value_counts(dropna=False) # dropna=False 옵션을 사용하지 않으면 유효한 데이터의 개수만을 구한다.\n",
    "nan_deck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0bbb8",
   "metadata": {},
   "source": [
    "누락 데이터를 찾는 직접적인 방법  \n",
    "* **isnull()** : 누락 데이터면 True 반환, 유효한 데이터가 존재하면 False 반환  \n",
    "* **notnull()** : 유효한 데이터가 존재하면 True 반환, 누락 데이터면 False 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6681af48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass    sex    age  sibsp  parch   fare  embarked  class  \\\n",
      "0     False   False  False  False  False  False  False     False  False   \n",
      "1     False   False  False  False  False  False  False     False  False   \n",
      "2     False   False  False  False  False  False  False     False  False   \n",
      "3     False   False  False  False  False  False  False     False  False   \n",
      "4     False   False  False  False  False  False  False     False  False   \n",
      "\n",
      "     who  adult_male   deck  embark_town  alive  alone  \n",
      "0  False       False   True        False  False  False  \n",
      "1  False       False  False        False  False  False  \n",
      "2  False       False   True        False  False  False  \n",
      "3  False       False  False        False  False  False  \n",
      "4  False       False   True        False  False  False  \n",
      "\n",
      "\n",
      "   survived  pclass   sex   age  sibsp  parch  fare  embarked  class   who  \\\n",
      "0      True    True  True  True   True   True  True      True   True  True   \n",
      "1      True    True  True  True   True   True  True      True   True  True   \n",
      "2      True    True  True  True   True   True  True      True   True  True   \n",
      "3      True    True  True  True   True   True  True      True   True  True   \n",
      "4      True    True  True  True   True   True  True      True   True  True   \n",
      "\n",
      "   adult_male   deck  embark_town  alive  alone  \n",
      "0        True  False         True   True   True  \n",
      "1        True   True         True   True   True  \n",
      "2        True  False         True   True   True  \n",
      "3        True   True         True   True   True  \n",
      "4        True  False         True   True   True  \n"
     ]
    }
   ],
   "source": [
    "print(df.head().isnull())\n",
    "print('\\n')\n",
    "print(df.head().notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c235c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "deck           3\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 누락 데이터의 개수 구하기 \n",
    "\n",
    "print(df.head().isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379aee3",
   "metadata": {},
   "source": [
    "* 누락 데이터 제거  \n",
    "\n",
    "dropna() 메소드 사용\n",
    "\n",
    "열을 삭제하면 분석 대상이 갖는 특성(변수)를 제거한다.  \n",
    "행을 삭제하면 분석 대상의 관측값(레코드)을 제거하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46161ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived : 0\n",
      "pclass : 0\n",
      "sex : 0\n",
      "age : 177\n",
      "sibsp : 0\n",
      "parch : 0\n",
      "fare : 0\n",
      "embarked : 2\n",
      "class : 0\n",
      "who : 0\n",
      "adult_male : 0\n",
      "deck : 688\n",
      "embark_town : 2\n",
      "alive : 0\n",
      "alone : 0\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "missing_df = df.isnull()\n",
    "for col in missing_df.columns:\n",
    "    missing_count = missing_df[col].value_counts() # 각 열의 NaN 개수 파악\n",
    "    \n",
    "    try: \n",
    "        print(col, \":\", missing_count[True])\n",
    "    except:\n",
    "        print(col, \":\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d8948",
   "metadata": {},
   "source": [
    "위 결과에서 'deck'열의 누락 데이터가 차지하는 비율이 매우 높기 때문에   \n",
    "'deck'열의 누락 데이터를 삭제해 분석에서 제외하는 것이 의미 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd9fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'embark_town', 'alive',\n",
      "       'alone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# NaN 값이 500개 이상인 열을 모두 삭제 - deck 열 삭제\n",
    "df_thresh = df.dropna(axis=1, thresh=500)\n",
    "print(df_thresh.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353d5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n"
     ]
    }
   ],
   "source": [
    "# age 열에 나이 데이터가 없는 모든 행 삭제\n",
    "df_age = df.dropna(subset=['age'], how='any', axis=0)\n",
    "print(len(df_age))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab73ae84",
   "metadata": {},
   "source": [
    "how='any' 옵션: NaN값이 하나라도 존재하면 삭제한다.(default)  \n",
    "how='all' 옵션: 모든 데이터가 NaN값일 경우에만 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a7db5",
   "metadata": {},
   "source": [
    "* 누락 데이터 치환  \n",
    "\n",
    "fillna() 메소드 사용  \n",
    "원본 객체를 변경하려면 inplace=True 옵션을 추가해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ca811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22.0\n",
       "1    38.0\n",
       "2    26.0\n",
       "3    35.0\n",
       "4    35.0\n",
       "5     NaN\n",
       "6    54.0\n",
       "7     2.0\n",
       "8    27.0\n",
       "9    14.0\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468d6584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22.000000\n",
      "1    38.000000\n",
      "2    26.000000\n",
      "3    35.000000\n",
      "4    35.000000\n",
      "5    29.699118\n",
      "6    54.000000\n",
      "7     2.000000\n",
      "8    27.000000\n",
      "9    14.000000\n",
      "Name: age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# NaN값을 다른 나이 데이터의 평균으로 변경\n",
    "mean_age = df['age'].mean() # 평균 계산\n",
    "df['age'].fillna(mean_age, inplace=True)\n",
    "\n",
    "print(df['age'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58910c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825     Queenstown\n",
       "826    Southampton\n",
       "827      Cherbourg\n",
       "828     Queenstown\n",
       "829            NaN\n",
       "Name: embark_town, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embark_town'][825:830]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f35888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southampton\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "825     Queenstown\n",
       "826    Southampton\n",
       "827      Cherbourg\n",
       "828     Queenstown\n",
       "829    Southampton\n",
       "Name: embark_town, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN값을 가장 많이 출현한 값으로 치환하기 (idxmax() 메소드 사용)\n",
    "most_freq = df['embark_town'].value_counts(dropna=True).idxmax() #최빈값 계산\n",
    "print(most_freq)\n",
    "\n",
    "df['embark_town'].fillna(most_freq, inplace=True)\n",
    "\n",
    "df['embark_town'][825:830]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d273f3",
   "metadata": {},
   "source": [
    "> **누락 데이터가 NaN으로 표시되지 않는 경우**\n",
    ">\n",
    "> 숫자 0이나 문자 '-', '?' 같은 값으로 입력되기도 한다.  \n",
    "> 판다스에서 누락 데이터를 다루려면 replace() 메소드를 활용해 NumPy에서 지원하는 np.nan으로 변경해주는 것이 좋다.\n",
    ">\n",
    "> * 사용법 예) `df.replace('?', np.nan, inplace=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15c3b0",
   "metadata": {},
   "source": [
    "데이터셋 특성상 서로 이웃하고 있는 데이터끼리 유사성을 가질 가능성이 높다.  \n",
    "\n",
    "fillna() 메소드에  \n",
    "method='ffill'('pad') 추가     : NaN이 있는 행의 직전 행에 있는 값으로 치환  \n",
    "method='bfill'('backfill') 추가 : NaN이 있는 행의 바로 다음 행에 있는 값으로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51697aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825     Queenstown\n",
       "826    Southampton\n",
       "827      Cherbourg\n",
       "828     Queenstown\n",
       "829            NaN\n",
       "Name: embark_town, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "df['embark_town'][825:830]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07d743fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825     Queenstown\n",
       "826    Southampton\n",
       "827      Cherbourg\n",
       "828     Queenstown\n",
       "829     Queenstown\n",
       "Name: embark_town, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embark_town'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "df['embark_town'][825:830]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16a4c9",
   "metadata": {},
   "source": [
    "## 2. 중복 데이터 처리\n",
    "\n",
    "* 중복 데이터 확인  \n",
    "\n",
    "duplicated() 메소드 사용  \n",
    "-> 이전 행들과 비교해 중복되는 행이면 True, 처음 나오는 행이면 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2caa3263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'c1':['a','a','b','a','b'],\n",
    "                  'c2':[1,1,1,2,2],\n",
    "                  'c3':[1,1,2,2,2]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a081ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "df_dup = df.duplicated()\n",
    "print(df_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d0ee776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: c2, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "col_dup = df['c2'].duplicated()\n",
    "print(col_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561ce1b",
   "metadata": {},
   "source": [
    "* 중복 데이터 제거  \n",
    "\n",
    "drop_duplicates() 메소드 사용  \n",
    "원본 객체를 변경하려면 inplace=True 옵션 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2ec92a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'c1':['a','a','b','a','b'],\n",
    "                  'c2':[1,1,1,2,2],\n",
    "                  'c3':[1,1,2,2,2]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b570a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop_duplicates()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1274f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n"
     ]
    }
   ],
   "source": [
    "# c2, c3열을 기준으로 중복 행 제거\n",
    "df3 = df.drop_duplicates(subset=['c2','c3'])\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a540e6",
   "metadata": {},
   "source": [
    "## 3. 데이터 표준화\n",
    "\n",
    "서로 다른 단위들이 섞여 있거나 같은 대상을 다른 형식으로 표현하는 등 동일한 대상을 표현하는 방법에 차이가 있으면, 분석의 정확도는 현저히 낮아진다.  \n",
    "따라서 데이터 포맷을 일관성 있게 표준화하는 작업이 필요하다.  \n",
    "\n",
    "### 3-1. 단위 환산  \n",
    "\n",
    "같은 데이터셋 안에서 측정 단위를 동일하게 맞춰야 일관성 측면에서 문제가 발생하지 않는다.  \n",
    "영미권에서 주로 사용하는 마일, 야드, 온스 등의 도량형 단위를 한국에서 사용하는 미터, 평, 그램 등으로 반환하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4a8036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('auto-mpg.csv', header=None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f157a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name       kpl  \n",
      "0       1  chevrolet chevelle malibu  7.652571  \n",
      "1       1          buick skylark 320  6.377143  \n",
      "2       1         plymouth satellite  7.652571  \n",
      "\n",
      "\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name   kpl  \n",
      "0       1  chevrolet chevelle malibu  7.65  \n",
      "1       1          buick skylark 320  6.38  \n",
      "2       1         plymouth satellite  7.65  \n"
     ]
    }
   ],
   "source": [
    "# mpg(mile per gallson)를 kpl(kilometer per liter)로 변환(1mpg = 0.425km/L)\n",
    "mpg_to_kpl = 1.60934/3.78541\n",
    "\n",
    "df['kpl'] = df['mpg'] * mpg_to_kpl\n",
    "print(df.head(3))\n",
    "print('\\n')\n",
    "df['kpl'] = df['kpl'].round(2) # 소수점 아래 둘째자리에서 반올림\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ea478",
   "metadata": {},
   "source": [
    "### 3-2. 자료형 변환\n",
    "\n",
    "먼저 dtypes 속성 혹은 info() 메소드로 각 열의 자료형을 확인 후   \n",
    "astype() 메소드를 사용해 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab746d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg             float64\n",
      "cylinders         int64\n",
      "displacement    float64\n",
      "horsepower       object\n",
      "weight          float64\n",
      "acceleration    float64\n",
      "model year        int64\n",
      "origin            int64\n",
      "name             object\n",
      "kpl             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a2d22",
   "metadata": {},
   "source": [
    "'horsepower' 열은 문자열을 뜻하는 object 자료형이다.  \n",
    "그러나 엔진 출력의 크기를 나타내는 데이터인 만큼 숫자형으로 변환해 주는 것이 적잘하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb4a7036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['130.0' '165.0' '150.0' '140.0' '198.0' '220.0' '215.0' '225.0' '190.0'\n",
      " '170.0' '160.0' '95.00' '97.00' '85.00' '88.00' '46.00' '87.00' '90.00'\n",
      " '113.0' '200.0' '210.0' '193.0' '?' '100.0' '105.0' '175.0' '153.0'\n",
      " '180.0' '110.0' '72.00' '86.00' '70.00' '76.00' '65.00' '69.00' '60.00'\n",
      " '80.00' '54.00' '208.0' '155.0' '112.0' '92.00' '145.0' '137.0' '158.0'\n",
      " '167.0' '94.00' '107.0' '230.0' '49.00' '75.00' '91.00' '122.0' '67.00'\n",
      " '83.00' '78.00' '52.00' '61.00' '93.00' '148.0' '129.0' '96.00' '71.00'\n",
      " '98.00' '115.0' '53.00' '81.00' '79.00' '120.0' '152.0' '102.0' '108.0'\n",
      " '68.00' '58.00' '149.0' '89.00' '63.00' '48.00' '66.00' '139.0' '103.0'\n",
      " '125.0' '133.0' '138.0' '135.0' '142.0' '77.00' '62.00' '132.0' '84.00'\n",
      " '64.00' '74.00' '116.0' '82.00']\n"
     ]
    }
   ],
   "source": [
    "print(df['horsepower'].unique())\n",
    "# 문자 '?'가 섞여 있어 문자열로 인식된 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c3a89d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# 누락 데이터('?') 삭제\n",
    "import numpy as np\n",
    "\n",
    "df['horsepower'].replace('?', np.nan, inplace=True)\n",
    "df.dropna(subset=['horsepower'], axis=0, inplace=True)\n",
    "\n",
    "# 문자열을 실수형으로 변환\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "print(df['horsepower'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e80c6b",
   "metadata": {},
   "source": [
    "'origin' 열은 정수형 데이터 1, 2, 3 을 갖고 있지만 실제로는 국가 이름인 'USA, EU, JPN' 을 뜻한다.  \n",
    "replace() 메소드를 사용해 각 숫자 데이터를 국가이름으로 바꿔주면 문자열을 나타내는 object 자료형으로 자동 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0495e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2]\n",
      "['USA' 'JPN' 'EU']\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df['origin'].unique())\n",
    "\n",
    "df['origin'].replace({1:'USA', 2:'EU', 3:'JPN'}, inplace=True)\n",
    "\n",
    "print(df['origin'].unique())\n",
    "print(df['origin'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d06b097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# 문자열을 범주형으로 변환\n",
    "df['origin'] = df['origin'].astype('category')\n",
    "print(df['origin'].dtypes)\n",
    "\n",
    "# 범주형을 다시 문자형으로 변환\n",
    "df['origin'] = df['origin'].astype('str')\n",
    "print(df['origin'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8e1fe",
   "metadata": {},
   "source": [
    "'model year' 열은 모델 출시년도가 숫자로 기록되어 있고 자료형은 정수형을 나타내는 int64이다.  \n",
    "연도는 시간적인 순서의 의민ㄴ 있으나 숫자의 상대적인 크기는 별 의미가 없다.  \n",
    "따라서 데이터는 숫자 형태를 갖더라도 자료형은 범주형으로 표현하는 것이 적절하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85eda577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      70\n",
      "128    74\n",
      "135    74\n",
      "Name: model year, dtype: int64\n",
      "\n",
      "\n",
      "26     70\n",
      "280    79\n",
      "226    77\n",
      "Name: model year, dtype: category\n",
      "Categories (13, int64): [70, 71, 72, 73, ..., 79, 80, 81, 82]\n"
     ]
    }
   ],
   "source": [
    "print(df['model year'].sample(3))\n",
    "print('\\n')\n",
    "df['model year'] = df['model year'].astype('category')\n",
    "print(df['model year'].sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005fd21",
   "metadata": {},
   "source": [
    "## 4. 범주형(카테고리) 데이터 처리\n",
    "\n",
    "### 4-1. 구간 분할\n",
    "\n",
    "구간 분할: 연속 변수를 일정한 구간으로 나누고, 각 구간을 범주형 이산 변수로 변환하는 과정  \n",
    "cut() 함수를 이용한다.  \n",
    "\n",
    "경계값은 histogram() 함수의 bins 옵션을 활용해 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f8c525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46.         107.33333333 168.66666667 230.        ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('auto-mpg.csv', header=None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "df['horsepower'].replace('?', np.nan, inplace=True)\n",
    "df.dropna(subset=['horsepower'], axis=0, inplace=True) # 누락 데이터 행 삭제\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "\n",
    "# 3개의 bin으로 나누는 경계값의 리스트 구하기\n",
    "count, bin_dividers = np.histogram(df['horsepower'], bins=3)\n",
    "print(bin_dividers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6789227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    horsepower hp_bin\n",
      "0        130.0   보통출력\n",
      "1        165.0   보통출력\n",
      "2        150.0   보통출력\n",
      "3        150.0   보통출력\n",
      "4        140.0   보통출력\n",
      "5        198.0    고출력\n",
      "6        220.0    고출력\n",
      "7        215.0    고출력\n",
      "8        225.0    고출력\n",
      "9        190.0    고출력\n",
      "10       170.0    고출력\n",
      "11       160.0   보통출력\n",
      "12       150.0   보통출력\n",
      "13       225.0    고출력\n",
      "14        95.0    저출력\n"
     ]
    }
   ],
   "source": [
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "\n",
    "# 각 데이터를 3개의 bin에 할당\n",
    "df['hp_bin'] = pd.cut(x=df['horsepower'],\n",
    "                     bins=bin_dividers,    # 경계값 리스트\n",
    "                     labels=bin_names,     # bin 이름\n",
    "                     include_lowest=True)  # 각 구간의 낮은 경계값(첫 경계값) 포함\n",
    "\n",
    "print(df[['horsepower', 'hp_bin']].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93775601",
   "metadata": {},
   "source": [
    "### 4-2. 더미 변수\n",
    "\n",
    "get_dummies() 함수 사용\n",
    "\n",
    "카테고리를 나타내는 범주형 데이터를 회귀분석 등 머신러닝 알고리즘에 바로 사용할 수 없는 경우가 있는데, 컴퓨터가 인식 가능한 입력값으로 변환해야 한다.  \n",
    "이럴 때 숫자 0 또는 1로 표현되는 더미 변수를 사용한다.  \n",
    "여기서 0과 1은 어떤 특성이 있는지 없는지 여부만을 표시한다.\n",
    "\n",
    "==> one-hot-coding 이라고도 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "052b8af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저출력  보통출력  고출력\n",
      "0     0     1    0\n",
      "1     0     1    0\n",
      "2     0     1    0\n",
      "3     0     1    0\n",
      "4     0     1    0\n",
      "5     0     0    1\n",
      "6     0     0    1\n",
      "7     0     0    1\n",
      "8     0     0    1\n",
      "9     0     0    1\n",
      "10    0     0    1\n",
      "11    0     1    0\n",
      "12    0     1    0\n",
      "13    0     0    1\n",
      "14    1     0    0\n"
     ]
    }
   ],
   "source": [
    "horsepower_dummies = pd.get_dummies(df['hp_bin'])\n",
    "print(horsepower_dummies.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c98748c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 0 1 1 0 2]\n",
      "<class 'numpy.ndarray'>\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]]\n",
      "<class 'numpy.ndarray'>\n",
      "  (0, 1)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 1)\t1.0\n",
      "  (12, 1)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 2)\t1.0\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# sklearn 라이브러리 이용해 원핫 인코딩 처리하기 ==> 희소행렬로 정리\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() # label encoder 생성\n",
    "onehot_encoder = preprocessing.OneHotEncoder() # one hot encoder 생성\n",
    "\n",
    "# label encoder로 문자열 범주를 숫자형 범주로 변환\n",
    "onehot_labeled = label_encoder.fit_transform(df['hp_bin'].head(15))\n",
    "print(onehot_labeled)\n",
    "print(type(onehot_labeled))\n",
    "\n",
    "# 2차원 행렬로 형태 변경\n",
    "onehot_reshaped = onehot_labeled.reshape(len(onehot_labeled), 1)\n",
    "print(onehot_reshaped)\n",
    "print(type(onehot_reshaped))\n",
    "\n",
    "# 희소행렬로 변환 -> (행, 열) 값\n",
    "onehot_fitted = onehot_encoder.fit_transform(onehot_reshaped)\n",
    "print(onehot_fitted)\n",
    "print(type(onehot_fitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f509e",
   "metadata": {},
   "source": [
    "## 5. 정규화\n",
    "\n",
    "숫자 데이터의 상대적인 크기 차이를 제거할 필요가 있다.  \n",
    "정규화: 각 열에 속하는 데이터 값을 동일한 크기 기준으로 나눈 비율로 나타내는 것  \n",
    "이 후 데이터의 범위는 0\\~1 또는 -1\\~1이 된다.\n",
    "\n",
    "1. 각 열의 데이터를 해당 열의 최대값으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68afe012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    392.000000\n",
      "mean     104.469388\n",
      "std       38.491160\n",
      "min       46.000000\n",
      "25%       75.000000\n",
      "50%       93.500000\n",
      "75%      126.000000\n",
      "max      230.000000\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "0    0.565217\n",
      "1    0.717391\n",
      "2    0.652174\n",
      "3    0.652174\n",
      "4    0.608696\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "count    392.000000\n",
      "mean       0.454215\n",
      "std        0.167353\n",
      "min        0.200000\n",
      "25%        0.326087\n",
      "50%        0.406522\n",
      "75%        0.547826\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('auto-mpg.csv', header=None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "df['horsepower'].replace('?', np.nan, inplace=True)\n",
    "df.dropna(subset=['horsepower'], axis=0, inplace=True) # 누락 데이터 행 삭제\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "\n",
    "print(df.horsepower.describe()) # 최대값 확인\n",
    "print('\\n')\n",
    "\n",
    "# horsepower 열의 최대값의 절대값으로 모든 데이터를 나눠서 저장\n",
    "df.horsepower = df.horsepower/abs(df.horsepower.max())\n",
    "\n",
    "print(df.horsepower.head())\n",
    "print('\\n')\n",
    "print(df.horsepower.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d5bfd",
   "metadata": {},
   "source": [
    "2. 각 열의 데이터 중에서 최대값과 최소값을 뺀 값으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "782ff041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.456522\n",
      "1    0.646739\n",
      "2    0.565217\n",
      "3    0.565217\n",
      "4    0.510870\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "count    392.000000\n",
      "mean       0.317768\n",
      "std        0.209191\n",
      "min        0.000000\n",
      "25%        0.157609\n",
      "50%        0.258152\n",
      "75%        0.434783\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# horsepower 열의 최대값의 절대값으로 모든 데이터를 나눠서 저장\n",
    "min_x = df.horsepower - df.horsepower.min()\n",
    "min_max = df.horsepower.max() - df.horsepower.min()\n",
    "df.horsepower = min_x/min_max\n",
    "\n",
    "print(df.horsepower.head())\n",
    "print('\\n')\n",
    "print(df.horsepower.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437ecae",
   "metadata": {},
   "source": [
    "## 6. 시계열 데이터\n",
    "\n",
    "시계열 데이터를 데이터프레임의 행 인덱스로 사용하면, 시간으로 기록된 데이터를 분석하는 것이 매우 편리하다.  \n",
    "판다스의 시간 표시 방식  \n",
    "**Timestamp**: 특정한 시점 기록  \n",
    "**Period**: 두 시점 사이의 일정한 기간 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349d9a0",
   "metadata": {},
   "source": [
    "## 6-1. 다른 자료형을 시계열 객체로 변환\n",
    "\n",
    "* 문자열을 Timestamp로 변환  \n",
    "\n",
    "판다스 to_datetime() 함수를 사용하면 문자열 등 다른 자료형을 판다스 Timestamp를 나타내는 datetime64 자료형으로 변환 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06845c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume\n",
      "0  2018-07-02  10100  10850  10900  10000  137977\n",
      "1  2018-06-29  10700  10550  10900   9990  170253\n",
      "2  2018-06-28  10400  10900  10950  10150  155769\n",
      "3  2018-06-27  10900  10800  11050  10500  133548\n",
      "4  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    20 non-null     object\n",
      " 1   Close   20 non-null     int64 \n",
      " 2   Start   20 non-null     int64 \n",
      " 3   High    20 non-null     int64 \n",
      " 4   Low     20 non-null     int64 \n",
      " 5   Volume  20 non-null     int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f79bb3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Date      20 non-null     object        \n",
      " 1   Close     20 non-null     int64         \n",
      " 2   Start     20 non-null     int64         \n",
      " 3   High      20 non-null     int64         \n",
      " 4   Low       20 non-null     int64         \n",
      " 5   Volume    20 non-null     int64         \n",
      " 6   new_Date  20 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5), object(1)\n",
      "memory usage: 1.2+ KB\n",
      "None\n",
      "\n",
      "\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())\n",
    "print('\\n')\n",
    "print(type(df['new_Date'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cc24dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Close  Start   High    Low  Volume\n",
      "new_Date                                      \n",
      "2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20 entries, 2018-07-02 to 2018-06-01\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Close   20 non-null     int64\n",
      " 1   Start   20 non-null     int64\n",
      " 2   High    20 non-null     int64\n",
      " 3   Low     20 non-null     int64\n",
      " 4   Volume  20 non-null     int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 960.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 시계열 값으로 변환된 열을 새로운 행 인덱스로 지정, 기존 날짜 열 삭제\n",
    "# ==> 시간 순서에 맞춰 인덱싱 또는 슬라이싱 하기가 편리하다.\n",
    "df.set_index('new_Date', inplace=True)\n",
    "df.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550403cd",
   "metadata": {},
   "source": [
    "* Timestamp를 Period로 변환  \n",
    "\n",
    "판다스 to_period() 함수를 이용하면 일정한 기간을 나타내는 Period 객체로 Timestamp 객체를 변환할 수 있다.  \n",
    "freq 옵션에 기준이 되는 기간을 설정한다.  \n",
    "  \n",
    "**freq 옵션의 종류**\n",
    "\n",
    "옵션  | 설명\n",
    "------|------\n",
    "    D | day(1일)\n",
    "    W | week(1주)\n",
    "    M | month end(월말)\n",
    "    MS| month begin(월초)\n",
    "    Q | quarter end(분기말)\n",
    "    QS| quarter begin(분기초)\n",
    "    A | year end(연말)\n",
    "    AS| year begin(연초)\n",
    "    B | business day(휴일 제외)\n",
    "    H | hour(1시간)\n",
    "    T | minute(1분)\n",
    "    S | second(1초)\n",
    "    L | milisecond\n",
    "    U | microsecond\n",
    "    N | nanosecond\n",
    "   ...| ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b79122f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "\n",
      "PeriodIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='period[D]')\n",
      "PeriodIndex(['2019-01', '2020-03', '2021-06'], dtype='period[M]')\n",
      "PeriodIndex(['2019', '2020', '2021'], dtype='period[A-DEC]')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dates = ['2019-01-01', '2020-03-01', '2021-06-01']\n",
    "\n",
    "# 문자열의 배열을 판다스 Timestamp로 변환\n",
    "ts_dates = pd.to_datetime(dates)\n",
    "print(ts_dates)\n",
    "print('\\n')\n",
    "\n",
    "# Timestamp를 Period로 변환\n",
    "pr_day = ts_dates.to_period(freq='D')\n",
    "print(pr_day)\n",
    "pr_month = ts_dates.to_period(freq='M')\n",
    "print(pr_month)\n",
    "pr_year = ts_dates.to_period(freq='A')\n",
    "print(pr_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b927968",
   "metadata": {},
   "source": [
    "### 6-2. 시계열 데이터 만들기\n",
    "\n",
    "* Timestamp 배열   \n",
    "\n",
    "판다스 date_range() 함수 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad53996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:00+09:00', '2019-02-01 00:00:00+09:00',\n",
      "               '2019-03-01 00:00:00+09:00', '2019-04-01 00:00:00+09:00',\n",
      "               '2019-05-01 00:00:00+09:00', '2019-06-01 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='MS')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ts_ms = pd.date_range(start='2019-01-01',\n",
    "                     end=None,         # 날짜 범위 끝\n",
    "                     periods=6,        # 생성할 Timestamp 개수\n",
    "                     freq='MS',        # 시간 간격\n",
    "                     tz='Asia/Seoul')  # 시간대(timezone)\n",
    "\n",
    "print(ts_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cee6568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-02-28 00:00:00+09:00',\n",
      "               '2019-03-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-05-31 00:00:00+09:00', '2019-06-30 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='M')\n"
     ]
    }
   ],
   "source": [
    "ts_me = pd.date_range('2019-01-01', periods=6,      \n",
    "                     freq='M',      \n",
    "                     tz='Asia/Seoul') \n",
    "print(ts_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99115fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-07-31 00:00:00+09:00', '2019-10-31 00:00:00+09:00',\n",
      "               '2020-01-31 00:00:00+09:00', '2020-04-30 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='3M')\n"
     ]
    }
   ],
   "source": [
    "# 분기 (3개월) 간격\n",
    "ts_3m = pd.date_range('2019-01-01', periods=6,      \n",
    "                     freq='3M',      \n",
    "                     tz='Asia/Seoul') \n",
    "print(ts_3m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2857e",
   "metadata": {},
   "source": [
    "* Period 배열  \n",
    "\n",
    "판다스 period_range() 함수는 여러 개의 기간이 들어 있는 시계열 데이터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1dcab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-31', '2019-02-28', '2019-03-31'], dtype='datetime64[ns]', freq='M')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pr_m = pd.date_range(start='2019-01-01', \n",
    "                     end=None,\n",
    "                     periods=3,      # 생성할 Period 개수\n",
    "                     freq='M')       # 기간의 길이(M: 월)\n",
    "print(pr_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6de1f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 01:00:00',\n",
      "               '2019-01-01 02:00:00'],\n",
      "              dtype='datetime64[ns]', freq='H')\n"
     ]
    }
   ],
   "source": [
    "pr_h = pd.date_range(start='2019-01-01', \n",
    "                     end=None,\n",
    "                     periods=3,     \n",
    "                     freq='H')    \n",
    "print(pr_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "864ebb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 02:00:00',\n",
      "               '2019-01-01 04:00:00'],\n",
      "              dtype='datetime64[ns]', freq='2H')\n"
     ]
    }
   ],
   "source": [
    "pr_2h = pd.date_range(start='2019-01-01', \n",
    "                      end=None,\n",
    "                      periods=3,     \n",
    "                      freq='2H')    \n",
    "print(pr_2h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca75af1",
   "metadata": {},
   "source": [
    "### 6-3. 시계열 데이터 활용\n",
    "\n",
    "* 날짜 데이터 분리\n",
    "\n",
    "1. dt 속성 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18177463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "\n",
      "\n",
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  Day\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7    2\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "\n",
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "\n",
    "df['Year'] = df['new_Date'].dt.year\n",
    "df['Month'] = df['new_Date'].dt.month\n",
    "df['Day'] = df['new_Date'].dt.day\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0646cc6",
   "metadata": {},
   "source": [
    "2. to_period() 메소드 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e47482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  \\\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7   \n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   \n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   \n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   \n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   \n",
      "\n",
      "   Day Date_yr   Date_m  \n",
      "0    2    2018  2018-07  \n",
      "1   29    2018  2018-06  \n",
      "2   28    2018  2018-06  \n",
      "3   27    2018  2018-06  \n",
      "4   26    2018  2018-06  \n"
     ]
    }
   ],
   "source": [
    "# Timestamp를 Period로 변환하여 연-월-일 표기 변경\n",
    "\n",
    "df['Date_yr'] = df['new_Date'].dt.to_period(freq='A')\n",
    "df['Date_m'] = df['new_Date'].dt.to_period(freq='M')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f64b360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Date  Close  Start   High    Low  Volume   new_Date  Year  \\\n",
      "Date_m                                                                     \n",
      "2018-07  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018   \n",
      "2018-06  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018   \n",
      "2018-06  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018   \n",
      "2018-06  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018   \n",
      "2018-06  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018   \n",
      "\n",
      "         Month  Day Date_yr  \n",
      "Date_m                       \n",
      "2018-07      7    2    2018  \n",
      "2018-06      6   29    2018  \n",
      "2018-06      6   28    2018  \n",
      "2018-06      6   27    2018  \n",
      "2018-06      6   26    2018  \n"
     ]
    }
   ],
   "source": [
    "df.set_index('Date_m', inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd750eb",
   "metadata": {},
   "source": [
    "* 날짜 인덱스 활용  \n",
    "\n",
    "시계열 데이터에 대한 인덱싱과 슬라이싱 편리하다.  \n",
    "내가 필요로 하는 레벨을 선택적으로 인덱싱할 수 있다. (연도, 연-월, 연-월-일...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "430ccff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "DatetimeIndex(['2018-07-02', '2018-06-29', '2018-06-28', '2018-06-27',\n",
      "               '2018-06-26', '2018-06-25', '2018-06-22', '2018-06-21',\n",
      "               '2018-06-20', '2018-06-19', '2018-06-18', '2018-06-15',\n",
      "               '2018-06-14', '2018-06-12', '2018-06-11', '2018-06-08',\n",
      "               '2018-06-07', '2018-06-05', '2018-06-04', '2018-06-01'],\n",
      "              dtype='datetime64[ns]', name='new_Date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "\n",
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('new_Date', inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4959aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "\n",
      "\n",
      "            Start   High\n",
      "new_Date                \n",
      "2018-07-02  10850  10900\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-06-25  2018-06-25  11150  11400  11450  11000   55519\n",
      "2018-06-22  2018-06-22  11300  11250  11450  10750  134805\n",
      "2018-06-21  2018-06-21  11200  11350  11750  11200  133002\n"
     ]
    }
   ],
   "source": [
    "df_y = df.loc['2018']\n",
    "print(df_y.head())\n",
    "print('\\n')\n",
    "df_ym = df.loc['2018-07']\n",
    "print(df_ym)\n",
    "print('\\n')\n",
    "df_ym_cols = df.loc['2018-07', 'Start':'High'] # 열 범위 슬라이싱\n",
    "print(df_ym_cols)\n",
    "print('\\n')\n",
    "df_ymd = df.loc['2018-07-02']\n",
    "print(df_ymd)\n",
    "print('\\n')\n",
    "df_ymd_range = df.loc['2018-06-20':'2018-06-25']\n",
    "print(df_ymd_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "625dca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "time_delta                                                \n",
      "180 days    2018-06-28  10400  10900  10950  10150  155769\n",
      "181 days    2018-06-27  10900  10800  11050  10500  133548\n",
      "182 days    2018-06-26  10800  10900  11000  10700   63039\n",
      "183 days    2018-06-25  11150  11400  11450  11000   55519\n",
      "186 days    2018-06-22  11300  11250  11450  10750  134805\n",
      "187 days    2018-06-21  11200  11350  11750  11200  133002\n",
      "188 days    2018-06-20  11550  11200  11600  10900  308596\n",
      "189 days    2018-06-19  11300  11850  11950  11300  180656\n"
     ]
    }
   ],
   "source": [
    "# 시간 간격 계산\n",
    "today = pd.to_datetime('2018-12-25')\n",
    "df['time_delta'] = today - df.index\n",
    "df.set_index('time_delta', inplace=True)\n",
    "df_180 = df['180 days':'189 days'] # 최근 180일 ~ 189일 사이의 값들 선택\n",
    "print(df_180)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
